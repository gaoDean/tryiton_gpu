This documentation covers the installation, architecture, usage, and technical details of the **FastFit Inference Engine**. This implementation is specifically optimized for headless cloud environments (e.g., Vast.ai) running Ubuntu with NVIDIA RTX 30-series GPUs.

-----

# FastFit Inference Engine Documentation

## 1. System Overview

This script is a robust wrapper around the FastFit virtual try-on diffusion model. It is designed to automate the complex preprocessing steps required for virtual try-on (Pose detection, Segmentation, Masking) and streamline the inference process into a single CLI command.

**Key Optimizations:**

  * **Headless-First:** Removes GUI dependencies, using OpenCV and PIL for image handling suitable for SSH terminals.
  * **VRAM Management:** Offloads the `DWPose` detector to the CPU to preserve GPU memory (VRAM) for the main diffusion process.
  * **VRAM Logging:** Automatically logs the maximum VRAM usage during inference for performance monitoring.
  * **Precision:** Uses `bfloat16` (Brain Floating Point) for the diffusion pipeline, offering a speed increase on Ampere (RTX 3090) cards without significant quality loss.
  * **Asset Management:** Automatically checks for and downloads required HuggingFace models (`FastFit-MR-1024`, `Human-Toolkit`) on the first run.

-----

## 2. Installation & Environment Setup

This setup assumes a fresh **Ubuntu** instance with **CUDA** drivers pre-installed (e.g., a standard Vast.ai template).

### Step 1: System Dependencies

Install the required system libraries for image processing (OpenCV) and version control.

```bash
apt-get update && apt-get install -y libgl1-mesa-glx libglib2.0-0 git
```

### Step 2: Repository Setup

Clone the base FastFit repository.

```bash
git clone https://github.com/Zheng-Chong/FastFit.git
cd FastFit
```

### Step 3: Python Dependencies

Create a `requirements.txt` file in the root directory with the exact versions below to ensure compatibility between `diffusers`, `torch`, and the FastFit pipeline.

**`requirements.txt` Content:**

```text
torch>=2.1.0
torchvision
diffusers>=0.24.0
transformers
accelerate
opencv-python
Pillow
numpy
huggingface_hub
scipy
safetensors
onnxruntime-gpu
```

**Install Commands:**

> **Note:** Detectron2 is installed directly via Git to ensure the pre-built wheels match the Linux/CUDA environment.

```bash
python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'
pip install -r requirements.txt
```

-----

## 3. Script Architecture (`inference.py`)

The script is built around the `FastFitEngine` class, which encapsulates the model lifecycle.

### Class: `FastFitEngine`

#### Initialization (`__init__`)

  * **Model Checks:** Verifies if models exist in the local `Models/` directory. If not, it triggers `snapshot_download` from HuggingFace.
  * **Pipeline Loading:**
      * **CPU:** Loads `DWPose` to system RAM.
      * **GPU:** Loads `DensePose`, `SCHP` (segmentation), and the main `FastFitPipeline`.

#### Pre-processing Logic

The script employs a `center_crop_to_aspect_ratio` helper function.

  * **Person Image:** Cropped to a 3:4 aspect ratio and resized to **768x1024**.
  * **Garments:** Resized to **768x1024** (Upper/Lower/Dress) or **384x512** (Accessories) to optimize token usage in the attention layers.

#### The Processing Loop (`process`)

1.  **Validation:** Checks if input files exist.
2.  **Feature Extraction:** Runs the person image through DWPose (Pose), DensePose (Shape), and SCHP (Body Segmentation).
3.  **Mask Generation:** Uses `multi_ref_cloth_agnostic_mask` to create a mask that tells the AI which parts of the image to repaint (the clothing areas) while preserving the face and hands.
4.  **Reference Loading:** Iterates through the garments dictionary. If a user does not provide a specific item (e.g., no bag), the script generates a black placeholder image and sets a "mask" value of `0` effectively telling the model to ignore that input slot.
5.  **Inference:** Runs the diffusion loop for the specified number of `steps` (default: 30) using a specific random `seed`.
6.  **Logging:** Reports the peak VRAM consumption during the inference phase (CUDA only).

-----

## 4. Usage Guide

Run the script from the root of the `FastFit` directory.

### Command Line Arguments

| Argument | Type | Required? | Description |
| :--- | :--- | :--- | :--- |
| `--person` | String | **Yes** | Path to the model/person image. |
| `--output` | String | No | Path for the saved result (Default: `output.png`). |
| `--upper` | String | No | Path to upper body garment (shirt, jacket). |
| `--lower` | String | No | Path to lower body garment (pants, skirt). |
| `--dress` | String | No | Path to a full-body dress. |
| `--shoe` | String | No | Path to footwear. |
| `--bag` | String | No | Path to a bag/purse. |
| `--steps` | Integer | No | Denoising steps. Higher = more detail, slower (Default: 30). |
| `--seed` | Integer | No | Seed for reproducibility. If not provided, a random seed will be used. |

### Important Logic Guards

The script contains a built-in conflict guard to prevent logical errors in the outfit construction:

> **Conflict Rule:** You cannot specify `--dress` at the same time as `--upper` or `--lower`.

### Examples

**1. Standard Outfit (Top & Bottom)**

```bash
python inference.py \
  --person data/model_01.jpg \
  --upper data/tshirt.jpg \
  --lower data/jeans.jpg \
  --output result_casual.png
```

**2. Full Body Dress with Heels**

```bash
python inference.py \
  --person data/model_02.jpg \
  --dress data/gown.jpg \
  --shoe data/heels.jpg \
  --steps 40 \
  --output result_formal.png
```

**3. Accessories Only (Changing shoes/bag on existing outfit)**
*Note: This is experimental usage. Usually, you want to include the clothes to maintain consistency.*

```bash
python inference.py \
  --person data/model_01.jpg \
  --upper data/current_shirt.jpg \
  --lower data/current_pants.jpg \
  --bag data/new_handbag.jpg \
  --output result_accessory.png
```

-----

## 5. Directory Structure

After running the script once, your directory structure on the server will look like this:

```text
FastFit/
├── Models/                     # Auto-created on first run
│   ├── FastFit-MR-1024/        # Main diffusion weights
│   └── Human-Toolkit/          # Preprocessors (DensePose, etc.)
├── module/                     # Core repo modules
├── inference.py                # This script
├── requirements.txt            # Dependency file
├── inputs/                     # (Suggested) Your source images
│   ├── model.jpg
│   └── shirt.jpg
└── output.png                  # Generated result
```

## 6. Troubleshooting

**Error: `OutOfMemoryError: CUDA out of memory`**

  * **Cause:** The 24GB VRAM buffer was exceeded.
  * **Solution:** The script handles this by putting `DWPose` on the CPU. If it persists, try closing other processes or reducing resolution constants in the script (`PERSON_SIZE = (576, 768)`).

**Error: `ModuleNotFoundError: No module named 'module'`**

  * **Cause:** The script is not running from the root of the FastFit repository.
  * **Solution:** Ensure you `cd FastFit` before running `python inference.py`.

**Error: `Configuration Conflict`**

  * **Cause:** You ran `--dress` with `--upper`.
  * **Solution:** Remove either the dress argument or the upper/lower arguments.

-----

## 7. Technical Documentation (Codebase Deep Dive)

This section provides an in-depth reference for the functions and classes orchestrated by `inference.py` and `server.py`, inspecting dependencies down to the utility and architectural level.

### `inference.py`

*   **`center_crop_to_aspect_ratio(img: Image.Image, target_ratio: float) -> Image.Image`**
    *   **Purpose:** Helper function to ensure input images match the model's expected dimensions.
    *   **Logic:** Calculates the center of the image and crops it to match `target_ratio` (e.g., 3:4) without distorting the aspect ratio.

*   **`class FastFitEngine`**
    *   **Purpose:** The primary controller class for the inference lifecycle.
    *   **`__init__`**: Initializes the computation device (CPU/CUDA) and verifies the existence of model weights (`FastFit-MR-1024` and `Human-Toolkit`). Calls `_load_pipeline` to load sub-models.
    *   **`_load_pipeline(mixed_precision)`**: Loads the three preprocessors (`DWPose`, `DensePose`, `SCHP`) and the main `FastFitPipeline`. Notably, `DWPose` is forced to CPU to save VRAM.
    *   **`process(person_path, garments, output_path, ...)`**:
        1.  **Image Loading:** Opens and crops the person image.
        2.  **Feature Extraction:** Generates Pose maps (skeleton), DensePose (IUV), and SCHP (segmentation) maps.
        3.  **Masking:** Calls `multi_ref_cloth_agnostic_mask` to generate the inpainting mask.
        4.  **Reference Prep:** Loads garment images, resizes them (smaller for accessories), and assigns them category labels (`upper`, `lower`, `overall`, etc.). Creates "blank" placeholders for missing items.
        5.  **Inference:** Calls `self.pipeline()` to generate the final image.
        6.  **Output:** Saves the result to disk.

*   **`main()`**
    *   **Purpose:** Command-line interface entry point. Parses arguments, enforcing conflict logic (e.g., preventing simultaneous `--dress` and `--upper`), and instantiates `FastFitEngine`.

### `server.py` (Flask API)

*   **`app`**: A Flask application instance that wraps `FastFitEngine`.
*   **`@app.route('/prompt', methods=['POST'])`**:
    *   **Input:** `multipart/form-data` containing:
        *   `person`: (Required) The target person image.
        *   `upper`, `lower`, `dress`, `shoe`, `bag`: (Optional) Garment images.
        *   `steps`: (Optional) Integer, default 30.
        *   `seed`: (Optional) Integer.
    *   **Logic:**
        1.  Creates a temporary directory for processing.
        2.  Validates inputs (checks for `person` and at least one garment).
        3.  Enforces outfit logic (Dress vs. Upper/Lower).
        4.  Calls `engine.process` with `output_path=None` to get a PIL image in memory.
        5.  Returns the generated image as a PNG response.
    *   **Error Handling:** Returns 400 for validation errors, 500 for processing errors.

### `module.pipeline_fastfit`

*   **`class FastFitPipeline`**
    *   **Purpose:** A specialized diffusion pipeline inheriting from `diffusers`, tailored for the FastFit architecture.
    *   **`__init__`**: Loads the core diffusion components: `AutoencoderKL` (VAE), `UNet2DConditionModel`, and `DDPMScheduler`. Optimizes performance using `torch.compile` and fused QKV projections.
    *   **`__call__`**: The main generation loop.
        *   **Latent Preparation:** Encodes the masked person image and reference garments into latent space using the VAE.
        *   **Conditioning:** Processes reference images and masks.
        *   **Attention Caching:** Pre-computes Key/Value pairs for the reference images to guide the UNet generation (using `cache_kv=True`).
        *   **Denoising Loop:** Iterates through `num_inference_steps`. For each step, it concatenates the noise, mask, and masked image latents, predicts noise residuals using the UNet, and updates the sample. It supports Classifier-Free Guidance (CFG) to improve adherence to the prompt/image prompts.
        *   **Decoding:** Decodes the final latents back into pixel space using the VAE and blends the result with the original image using the mask (inpainting).

### `module.utils` (Image & Tensor Utilities)

*   **`paste_image_back_with_feathering`**:
    *   **Purpose:** Blends a generated/inpainted crop back into the original full-resolution image.
    *   **Technique:** Creates a mask matching the crop area, applies a Gaussian blur (`feather_radius`), and composites the images using this alpha mask for a seamless transition.
*   **`adjust_input_image`**:
    *   **Purpose:** Prepares the input image for inpainting by centering on the mask.
    *   **Logic:** Finds the bounding box of the mask, adjusts it to match the target aspect ratio, adds padding (`padding_ratio`), and crops the image/mask to this new viewport.
*   **`prepare_image` / `prepare_mask_image`**:
    *   **Purpose:** Converts PIL images or Numpy arrays into PyTorch tensors.
    *   **Normalization:** `prepare_image` normalizes pixel values to `[-1, 1]` (standard for latent diffusion). `prepare_mask_image` binarizes the mask to `{0, 1}`.
*   **`get_bounding_box`**: Calculates the `(xmin, ymin, xmax, ymax)` of non-zero pixels in a mask.
*   **`resize_and_crop` / `resize_and_padding`**: Helpers for image resizing strategies (filling vs. fitting).

### `module.attention_processor` (Attention Mechanism)

*   **`class OptimizedAttentionCache`**:
    *   **Purpose:** Implements the "FastFit" efficiency mechanism. Instead of re-computing reference features at every step, it caches the **Keys** and **Values** from the reference garment images.
    *   **Storage:** Pre-allocates tensors on the GPU for `cached_keys`, `cached_values`, and `cached_mask`.
*   **`class AttnProcessor2_0` (and `Fused` variant)**:
    *   **Purpose:** Overrides standard PyTorch attention to support KV Caching.
    *   **Cache Logic:**
        *   If `cache_kv=True` (used during the "ref" pass), it computes K/V pairs and stores them in `OptimizedAttentionCache`.
        *   If `cache_kv=False` (used during the "denoising" pass), it retrieves the cached K/V pairs from the reference images, concatenates them with the current hidden states, and performs attention. This allows the model to "attend" to the reference garment details without recalculating them repeatedly.

### `parse_utils.automasker` (Mask Generation)

*   **`multi_ref_cloth_agnostic_mask`**:
    *   **Purpose:** The primary masker for inference.
    *   **Logic:** Unions the segmentation maps of all clothing items (Top, Bottom, Dress, Coat, Shoes, etc.) from the SCHP parsers. It excludes the **Face** region (via `schp_lip_mask` Face label) to protect identity. It explicitly *disables* strong hand protection to allow sleeves/gloves to be generated naturally.
*   **`cloth_agnostic_mask`**:
    *   **Purpose:** A more granular masker for specific body parts.
    *   **Logic:** Accepts a `part` argument (`upper`, `lower`, `overall`). It employs **Strong Protection** for Hands and Feet using DensePose data (`DENSE_INDEX_MAP`) to prevent the model from painting over extremities.
*   **`part_mask_of`**:
    *   **Purpose:** Extracts binary masks for specific semantic labels (e.g., "Upper-clothes") from the raw segmentation arrays using `ATR_MAPPING` or `LIP_MAPPING`.
*   **`create_bounding_box_mask`**:
    *   **Purpose:** Creates a rectangular mask around a target area, optionally expanding it horizontally. Useful for "square" inpainting tasks.
*   **`hull_mask`**:
    *   **Purpose:** Computes the Convex Hull of a mask to fill in holes or gaps in the segmentation map.

### `parse_utils.dwpose`

*   **`class DWposeDetector`**
    *   **Purpose:** Wrapper for the Wholebody pose estimation model (YOLOX + DWPose).
    *   **`__init__`**: Loads the ONNX models for detection and pose estimation.
    *   **`__call__`**: Accepts an image, resizes it, runs inference to detect body keypoints (body, hands, face), and draws the "skeleton" pose image used to condition the diffusion model.

### `parse_utils.densepose.pipeline`

*   **`class DensePose`**
    *   **Purpose:** Wrapper for Facebook's Detectron2 DensePose model.
    *   **`__init__`**: Configures the Detectron2 predictor with the specific config (`densepose_rcnn_R_50_FPN_s1x.yaml`) and weights.
    *   **`__call__`**: Runs inference to map pixels of the person to a 3D surface coordinate system (IUV). Returns a visualized DensePose map (often purple/green) that helps the model understand body shape and orientation.

### `parse_utils.schp`

*   **`class SCHP`**
    *   **Purpose:** Self-Correction for Human Parsing. Used to segment the image into semantic regions (e.g., "this pixel is hair", "this pixel is a shirt").
    *   **`__init__`**: Initializes a ResNet101-based segmentation model tailored for datasets like LIP (Look Into Person) or ATR.
    *   **`preprocess`**: Applies affine transformations and normalization to prepare the input image for the network.
    *   **`__call__`**: Runs the model to produce a segmentation map where each pixel value corresponds to a specific category (Face, Upper-clothes, Pants, etc.).

-----

## 8. Update Log

### December 1, 2025
*   **Initial Documentation Generation:**
    *   Created extensive documentation for `inference.py` and its immediate dependencies, including `module.pipeline_fastfit`, `parse_utils.dwpose`, `parse_utils.densepose.pipeline`, `parse_utils.schp`, and `parse_utils.automasker`.
    *   Added details on the Flask API (`server.py`).
    *   Documented key utility functions in `module/utils.py`.
    *   Provided insights into the attention caching mechanism within `module/attention_processor.py`.
    *   Expanded on the mask generation logic in `parse_utils/automasker.py`, differentiating between `multi_ref_cloth_agnostic_mask` and `cloth_agnostic_mask`.
    *   Organized this information under "7. Technical Documentation (Codebase Deep Dive)".