This documentation covers the installation, architecture, usage, and technical details of the **FastFit Inference Engine**. This implementation is specifically optimized for headless cloud environments (e.g., Vast.ai) running Ubuntu with NVIDIA RTX 30-series GPUs.

-----

# FastFit Inference Engine Documentation

## 1\. System Overview

This script is a robust wrapper around the FastFit virtual try-on diffusion model. It is designed to automate the complex preprocessing steps required for virtual try-on (Pose detection, Segmentation, Masking) and streamline the inference process into a single CLI command.

**Key Optimizations:**

  * **Headless-First:** Removes GUI dependencies, using OpenCV and PIL for image handling suitable for SSH terminals.
  * **VRAM Management:** Offloads the `DWPose` detector to the CPU to preserve GPU memory (VRAM) for the main diffusion process.
  * **VRAM Logging:** Automatically logs the maximum VRAM usage during inference for performance monitoring.
  * **Precision:** Uses `bfloat16` (Brain Floating Point) for the diffusion pipeline, offering a speed increase on Ampere (RTX 3090) cards without significant quality loss.
  * **Asset Management:** Automatically checks for and downloads required HuggingFace models (`FastFit-MR-1024`, `Human-Toolkit`) on the first run.

-----

## 2\. Installation & Environment Setup

This setup assumes a fresh **Ubuntu** instance with **CUDA** drivers pre-installed (e.g., a standard Vast.ai template).

### Step 1: System Dependencies

Install the required system libraries for image processing (OpenCV) and version control.

```bash
apt-get update && apt-get install -y libgl1-mesa-glx libglib2.0-0 git
```

### Step 2: Repository Setup

Clone the base FastFit repository.

```bash
git clone https://github.com/Zheng-Chong/FastFit.git
cd FastFit
```

### Step 3: Python Dependencies

Create a `requirements.txt` file in the root directory with the exact versions below to ensure compatibility between `diffusers`, `torch`, and the FastFit pipeline.

**`requirements.txt` Content:**

```text
torch>=2.1.0
torchvision
diffusers>=0.24.0
transformers
accelerate
opencv-python
Pillow
numpy
huggingface_hub
scipy
safetensors
onnxruntime-gpu
```

**Install Commands:**

> **Note:** Detectron2 is installed directly via Git to ensure the pre-built wheels match the Linux/CUDA environment.

```bash
python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'
pip install -r requirements.txt
```

-----

## 3\. Script Architecture (`inference.py`)

The script is built around the `FastFitEngine` class, which encapsulates the model lifecycle.

### Class: `FastFitEngine`

#### Initialization (`__init__`)

  * **Model Checks:** Verifies if models exist in the local `Models/` directory. If not, it triggers `snapshot_download` from HuggingFace.
  * **Pipeline Loading:**
      * **CPU:** Loads `DWPose` to system RAM.
      * **GPU:** Loads `DensePose`, `SCHP` (segmentation), and the main `FastFitPipeline`.

#### Pre-processing Logic

The script employs a `center_crop_to_aspect_ratio` helper function.

  * **Person Image:** Cropped to a 3:4 aspect ratio and resized to **768x1024**.
  * **Garments:** Resized to **768x1024** (Upper/Lower/Dress) or **384x512** (Accessories) to optimize token usage in the attention layers.

#### The Processing Loop (`process`)

1.  **Validation:** Checks if input files exist.
2.  **Feature Extraction:** Runs the person image through DWPose (Pose), DensePose (Shape), and SCHP (Body Segmentation).
3.  **Mask Generation:** Uses `multi_ref_cloth_agnostic_mask` to create a mask that tells the AI which parts of the image to repaint (the clothing areas) while preserving the face and hands.
4.  **Reference Loading:** Iterates through the garments dictionary. If a user does not provide a specific item (e.g., no bag), the script generates a black placeholder image and sets a "mask" value of `0` effectively telling the model to ignore that input slot.
5.  **Inference:** Runs the diffusion loop for the specified number of `steps` (default: 30) using a specific random `seed`.
6.  **Logging:** Reports the peak VRAM consumption during the inference phase (CUDA only).

-----

## 4\. Usage Guide

Run the script from the root of the `FastFit` directory.

### Command Line Arguments

| Argument | Type | Required? | Description |
| :--- | :--- | :--- | :--- |
| `--person` | String | **Yes** | Path to the model/person image. |
| `--output` | String | No | Path for the saved result (Default: `output.png`). |
| `--upper` | String | No | Path to upper body garment (shirt, jacket). |
| `--lower` | String | No | Path to lower body garment (pants, skirt). |
| `--dress` | String | No | Path to a full-body dress. |
| `--shoe` | String | No | Path to footwear. |
| `--bag` | String | No | Path to a bag/purse. |
| `--steps` | Integer | No | Denoising steps. Higher = more detail, slower (Default: 30). |
| `--seed` | Integer | No | Seed for reproducibility. If not provided, a random seed will be used. |

### Important Logic Guards

The script contains a built-in conflict guard to prevent logical errors in the outfit construction:

> **Conflict Rule:** You cannot specify `--dress` at the same time as `--upper` or `--lower`.

### Examples

**1. Standard Outfit (Top & Bottom)**

```bash
python inference.py \
  --person data/model_01.jpg \
  --upper data/tshirt.jpg \
  --lower data/jeans.jpg \
  --output result_casual.png
```

**2. Full Body Dress with Heels**

```bash
python inference.py \
  --person data/model_02.jpg \
  --dress data/gown.jpg \
  --shoe data/heels.jpg \
  --steps 40 \
  --output result_formal.png
```

**3. Accessories Only (Changing shoes/bag on existing outfit)**
*Note: This is experimental usage. Usually, you want to include the clothes to maintain consistency.*

```bash
python inference.py \
  --person data/model_01.jpg \
  --upper data/current_shirt.jpg \
  --lower data/current_pants.jpg \
  --bag data/new_handbag.jpg \
  --output result_accessory.png
```

-----

## 5\. Directory Structure

After running the script once, your directory structure on the server will look like this:

```text
FastFit/
├── Models/                     # Auto-created on first run
│   ├── FastFit-MR-1024/        # Main diffusion weights
│   └── Human-Toolkit/          # Preprocessors (DensePose, etc.)
├── module/                     # Core repo modules
├── inference.py                # This script
├── requirements.txt            # Dependency file
├── inputs/                     # (Suggested) Your source images
│   ├── model.jpg
│   └── shirt.jpg
└── output.png                  # Generated result
```

## 6\. Troubleshooting

**Error: `OutOfMemoryError: CUDA out of memory`**

  * **Cause:** The 24GB VRAM buffer was exceeded.
  * **Solution:** The script handles this by putting `DWPose` on the CPU. If it persists, try closing other processes or reducing resolution constants in the script (`PERSON_SIZE = (576, 768)`).

**Error: `ModuleNotFoundError: No module named 'module'`**

  * **Cause:** The script is not running from the root of the FastFit repository.
  * **Solution:** Ensure you `cd FastFit` before running `python inference.py`.

**Error: `Configuration Conflict`**

  * **Cause:** You ran `--dress` with `--upper`.
  * **Solution:** Remove either the dress argument or the upper/lower arguments.

**Next Step:** Would you like me to write a small bash script that automatically watches an `inputs/` folder and processes images as soon as you upload them (batch processing)?